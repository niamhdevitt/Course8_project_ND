---
output:
  html_document: default
  pdf_document: default
---
### Course 8 project
## Niamh Devitt

## Analysis of the quality of exercise taken, using data from a number of different exercise recording devices

# Executive Summary 
This report explores not what kind of activity is taken but how well it is taken. To do this I've used data on 6 participants who were asked to do a series of exercises correctly and incorrectly.


# Data Cleaning and Exploration

First we download the training and test datasets
```{r getdata}
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url_train, destfile = "train_data.csv")
download.file(url_test, destfile = "test_data.csv")

train_data <- read.csv("~/DataScience/Course8_PracticalMachineLearning/train_data.csv")
test_data <- read.csv("~/DataScience/Course8_PracticalMachineLearning/test_data.csv")
```

Data cleaning; we remove a number of variables with near zero variance that will have little impact in our prediction. First 7 columns are not predictors and we'll remove columns that are mostly blank before we build our models.

```{r cleaning}
train_clean <- train_data[,-c(1:7)]
remove_blanks <- which(colSums(is.na(train_clean) |train_clean=="")>0.9*dim(train_clean)[1]) 
train_clean <- train_clean[,-remove_blanks]

test_clean <- test_data[,-c(1,7)]
remove_blanks2 <- which(colSums(is.na(test_clean) |test_clean=="")>0.9*dim(test_clean)[1]) 
test_clean <- test_clean[,-remove_blanks2]

```

First we will split the training data into a training dataset and a validation set where we will test our final model before applying to the test set and submitting.

```{r data_exp}
set.seed(383)
inTrain <- createDataPartition(train_clean$classe, p=0.7, list= FALSE)
training <- train_clean[inTrain,]
validation <- train_clean[-inTrain,]

dim(training)
dim(validation)
levels(training$classe)
```

# Model Building

In order to avoid overfitting and to assess the effectiveness of our models we will use cross-validation in all of our models. We will fit a number of models and assess their accuracy giving us a sense of their out of sample error rate.

# Model 1: Classification Tree
```{r CtModel}
set.seed(111)
mod_cv <- trainControl(method="cv", number=3, verboseIter=FALSE)
modfit_ct<- train(classe~., data=training, method="rpart", trControl=mod_cv)
modfit_ct$finalModel

rattle::fancyRpartPlot(modfit_ct$finalModel)
```

# Model 2: Random Forest
```{r rfmodel}
set.seed(456)
mod_cv <- trainControl(method="cv", number=3, verboseIter=FALSE)
modfit_rf <- train(classe ~., data = training, method = "rf", trControl = mod_cv)
modfit_rf$finalModel
```

# Model 3: Gradient Boosting

```{r gbmModel}
set.seed(444)
modfit_gbm <- train(classe ~., data = training, method = "gbm", verbose=FALSE, trControl = mod_cv)
modfit_gbm$finalModel
```


# Model Evaluation and Selection

The accuracy of our classification tree is low at 49.5%. This means in-sample error is c.50% and we can expect out of sample error to be worse meaing this is model than random selection.

The random forest model has 99.2% accuracy with cross validation 3 times in predicting the validation set, expected out of sample error will be higher than in-sample error so will be slightly bigger than 1% in sample error however this is still a very strong model.

The gradient boosting model has 96% accuracy with cross validation. Our final model will be the random forest model.

```{r selection}
pred_ct <- predict(modfit_ct, newdata=validation)
acc_ct <- confusionMatrix(pred_ct, validation$classe)$overall['Accuracy']

pred_rf <- predict(modfit_rf, newdata=validation)
acc_rf <- confusionMatrix(pred_rf, validation$classe)$overall['Accuracy']

pred_gbm <- predict(modfit_gbm, newdata=validation)
acc_gbm <- confusionMatrix(pred_gbm, validation$classe)$overall['Accuracy']

accuracy_vec <- c(acc_ct, acc_rf, acc_gbm)
names(accuracy_vec) <- c("classification", "random forest", "gradient boost")
accuracy_vec
```

We will look at variable importance for our selected model. We can see high importance in the top 8 predictor before a significant drop. We could streamline the predictors if we wanted.

```{r varimp}
rfImp <- varImp(modfit_rf, scale = FALSE)
plot(rfImp)
```

# Using our Final Model to predict on the Clean Test set
```{r finaltest}
FinalTest_pred <- predict(modfit_rf,newdata=test_clean)
FinalTest_pred
```
The results wll be tested in Course Project Quiz.